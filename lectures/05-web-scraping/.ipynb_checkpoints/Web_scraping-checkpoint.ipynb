{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation\n",
    "\n",
    "With conda, you can install the required dependencies with:\n",
    "\n",
    "```bash\n",
    "conda install bs4 requests lxml\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic usage of BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we import the `BeatifulSoup` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the html source file from disk and pass the source the the BeautifulSoup constructor. We choose the \"lxml\" parser for XML documents, which is faster than the defaul parser that comes with BeautifulSoup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html>\n",
      "<body>\n",
      "<h2>An Unordered HTML List</h2>\n",
      "<ul id=\"unordered_list\" style=\"color:#069\">\n",
      "<li>Coffee</li>\n",
      "<li>Tea</li>\n",
      "<li>Milk</li>\n",
      "</ul>\n",
      "<h2>An Ordered HTML List</h2>\n",
      "<ol id=\"ordered_list\" style=\"color:#069\">\n",
      "<li>Coffee</li>\n",
      "<li>Tea</li>\n",
      "<li>Milk</li>\n",
      "</ol>\n",
      "</body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "src = open(\"list.html\")\n",
    "document = BeautifulSoup(src, 'lxml')\n",
    "print(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding tags by name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The document now contains the full html document. We can find the first occuring tag with a specific name with the `find` function. Let's find the first un-ordered list tag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ulist = document.find(\"ul\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result contains all tags contained in the matched tag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ul id=\"unordered_list\" style=\"color:#069\">\n",
       "<li>Coffee</li>\n",
       "<li>Tea</li>\n",
       "<li>Milk</li>\n",
       "</ul>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ulist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `find_all` function returns **all** tags that match the given tag name. We can use it to get a list of all list items:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<li>Coffee</li>, <li>Tea</li>, <li>Milk</li>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items = ulist.find_all(\"li\")\n",
    "items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can loop over all items and extract their contant with the `get_text` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coffee\n",
      "Tea\n",
      "Milk\n"
     ]
    }
   ],
   "source": [
    "for item in items:\n",
    "    print(item.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `find_all` is **recursive** by default. This means that we could call it the on the full `document` to get the items\n",
    "of both the ordered and un-ordered lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document.find_all(\"li\", recursive=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding tags by attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes the easiest way to find a tag is by its attribute name. In our examples, both lists have an `id` attribute that uniquely identifies the tables. We can also use the `find*` methods to search for attributes:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ul id=\"unordered_list\" style=\"color:#069\">\n",
       "<li>Coffee</li>\n",
       "<li>Tea</li>\n",
       "<li>Milk</li>\n",
       "</ul>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document.find(attrs={\"id\":\"unordered_list\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ul` tag also contains a `style` attribute. Any bs4 tag behaves like a dictionary with attribute names as keys and attribute values as values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'unordered_list', 'style': 'color:#069'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ulist.attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'color:#069'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ulist[\"style\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading a table from Wikipedia\n",
    "\n",
    "We aim to get a list of countries sorted by their population size:\n",
    "https://en.wikipedia.org/wiki/List_of_countries_and_dependencies_by_population"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's import the required modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import dateutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, we load the html source directly from a website using the requests module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = requests.get(\"https://en.wikipedia.org/wiki/List_of_countries_and_dependencies_by_population\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The web server returns a status code to indicate if the request was (un-)succesfully.\n",
    "We use that status-code to check if the page was succesfully loaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert result.status_code==200  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we extract the html source and initiated BeautifulSoup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = result.content\n",
    "document = BeautifulSoup(src, 'lxml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "by looking at the document, we can see that we are interested in first table. So we use `find`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = document.find(\"table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are not familiar with html table, read this example first: https://www.w3schools.com/html/tryit.asp?filename=tryhtml_table_intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, it is a good idea to programatically check that the table contains the correct header:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert table.find(\"th\").get_text() == \"Rank\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = table.find_all(\"tr\")  # Note: this works because find_all is resursive by default\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 China \t\t Asia\n",
      "2 India \t\t Asia\n",
      "3 United States \t\t North America\n",
      "4 Indonesia \t\t Asia[b]\n",
      "5 Pakistan \t\t Asia\n",
      "6 Nigeria \t\t Africa\n",
      "7 Brazil \t\t South America\n",
      "8 Bangladesh \t\t Asia\n",
      "9 Russia \t\t Europe[c]\n",
      "10 Mexico \t\t North America\n",
      "11 Japan \t\t Asia\n",
      "12 Philippines \t\t Asia\n",
      "13 Ethiopia \t\t Africa\n",
      "14 Egypt \t\t Africa[b]\n",
      "15 Vietnam \t\t Asia\n",
      "16 DR Congo \t\t Africa\n",
      "17 Iran \t\t Asia\n",
      "18 Turkey \t\t Asia[b]\n",
      "19 Germany \t\t Europe\n",
      "20 France \t\t Europe\n",
      "21 United Kingdom \t\t Europe\n",
      "22 Thailand \t\t Asia\n",
      "23 Tanzania \t\t Africa\n",
      "24 South Africa \t\t Africa\n",
      "25 Italy \t\t Europe\n",
      "26 Myanmar \t\t Asia\n",
      "27 South Korea \t\t Asia\n",
      "28 Colombia \t\t South America\n",
      "29 Kenya \t\t Africa\n",
      "30 Spain \t\t Europe\n",
      "31 Argentina \t\t South America\n",
      "32 Algeria \t\t Africa\n",
      "33 Sudan \t\t Africa\n",
      "34 Uganda \t\t Africa\n",
      "35 Iraq \t\t Asia\n",
      "36 Ukraine \t\t Europe\n",
      "37 Canada \t\t North America\n",
      "38 Poland \t\t Europe\n",
      "39 Morocco \t\t Africa\n",
      "40 Uzbekistan \t\t Asia\n",
      "41 Saudi Arabia \t\t Asia\n",
      "42 Peru \t\t South America\n",
      "43 Angola \t\t Africa\n",
      "44 Afghanistan \t\t Asia\n",
      "45 Malaysia \t\t Asia\n",
      "46 Mozambique \t\t Africa\n",
      "47 Yemen \t\t Asia\n",
      "48 Ghana \t\t Africa\n",
      "49 Ivory Coast \t\t Africa\n",
      "50 Venezuela \t\t South America\n",
      "51 Nepal \t\t Asia\n",
      "52 Madagascar \t\t Africa\n",
      "53 Australia \t\t Oceania\n",
      "54 North Korea \t\t Asia\n",
      "55 Cameroon \t\t Africa\n",
      "56 Niger \t\t Africa\n",
      "– Taiwan \t\t Asia\n",
      "57 Sri Lanka \t\t Asia\n",
      "58 Burkina Faso \t\t Africa\n",
      "59 Malawi \t\t Africa\n",
      "60 Mali \t\t Africa\n",
      "61 Chile \t\t South America\n",
      "62 Kazakhstan \t\t Asia[b]\n",
      "63 Romania \t\t Europe\n",
      "64 Zambia \t\t Africa\n",
      "65 Syria \t\t Asia\n",
      "66 Ecuador \t\t South America\n",
      "67 Netherlands \t\t Europe\n",
      "68 Senegal \t\t Africa\n",
      "69 Guatemala \t\t North America\n",
      "70 Chad \t\t Africa\n",
      "71 Somalia \t\t Africa\n",
      "72 Cambodia \t\t Asia\n",
      "73 Zimbabwe \t\t Africa\n",
      "74 South Sudan \t\t Africa\n",
      "75 Rwanda \t\t Africa\n",
      "76 Guinea \t\t Africa\n",
      "77 Burundi \t\t Africa\n",
      "78 Benin \t\t Africa\n",
      "79 Tunisia \t\t Africa\n",
      "80 Bolivia \t\t South America\n",
      "81 Haiti \t\t North America\n",
      "82 Belgium \t\t Europe\n",
      "83 Jordan \t\t Asia\n",
      "84 Cuba \t\t North America\n",
      "85 Dominican Republic \t\t North America\n",
      "86 Czech Republic \t\t Europe\n",
      "87 Sweden \t\t Europe\n",
      "88 Greece \t\t Europe\n",
      "89 Portugal \t\t Europe\n",
      "90 Azerbaijan \t\t Asia[b]\n",
      "91 Hungary \t\t Europe\n",
      "92 Israel \t\t Asia\n",
      "93 Honduras \t\t North America\n",
      "94 Tajikistan \t\t Asia\n",
      "95 United Arab Emirates \t\t Asia\n",
      "96 Belarus \t\t Europe\n",
      "97 Papua New Guinea \t\t Oceania\n",
      "98 Austria \t\t Europe\n",
      "99 Switzerland \t\t Europe\n",
      "100 Sierra Leone \t\t Africa\n",
      "101 Togo \t\t Africa\n",
      "– Hong Kong(China) \t\t Asia\n",
      "102 Paraguay \t\t South America\n",
      "103 Laos \t\t Asia\n",
      "104 Libya \t\t Africa\n",
      "105 El Salvador \t\t North America\n",
      "106 Serbia \t\t Europe\n",
      "107 Lebanon \t\t Asia\n",
      "108 Kyrgyzstan \t\t Asia\n",
      "109 Nicaragua \t\t North America\n",
      "110 Bulgaria \t\t Europe\n",
      "111 Turkmenistan \t\t Asia\n",
      "112 Denmark \t\t Europe\n",
      "113 Congo \t\t Africa\n",
      "114 Central African Republic \t\t Africa\n",
      "115 Finland \t\t Europe\n",
      "116 Singapore \t\t Asia\n",
      "117 Norway \t\t Europe\n",
      "118 Slovakia \t\t Europe\n",
      "119 Palestine \t\t Asia\n",
      "120 Costa Rica \t\t North America\n",
      "121 New Zealand \t\t Oceania\n",
      "122 Ireland \t\t Europe\n",
      "123 Kuwait \t\t Asia\n",
      "124 Liberia \t\t Africa\n",
      "125 Oman \t\t Asia\n",
      "126 Panama \t\t North America\n",
      "127 Mauritania \t\t Africa\n",
      "128 Croatia \t\t Europe\n",
      "129 Georgia \t\t Asia[b]\n",
      "130 Eritrea \t\t Africa\n",
      "131 Uruguay \t\t South America\n",
      "132 Mongolia \t\t Asia\n",
      "133 Bosnia and Herzegovina \t\t Europe\n",
      "– Puerto Rico(United States) \t\t North America\n",
      "134 Armenia \t\t Asia\n",
      "135 Lithuania \t\t Europe\n",
      "136 Albania \t\t Europe\n",
      "137 Qatar \t\t Asia\n",
      "138 Jamaica \t\t North America\n",
      "139 Moldova \t\t Europe\n",
      "140 Namibia \t\t Africa\n",
      "141 Gambia \t\t Africa\n",
      "142 Botswana \t\t Africa\n",
      "143 Gabon \t\t Africa\n",
      "144 Lesotho \t\t Africa\n",
      "145 Slovenia \t\t Europe\n",
      "146 Latvia \t\t Europe\n",
      "147 North Macedonia \t\t Europe\n",
      "– Kosovo \t\t Europe\n",
      "148 Guinea-Bissau \t\t Africa\n",
      "149 Equatorial Guinea \t\t Africa\n",
      "150 Bahrain \t\t Asia\n",
      "151 Trinidad and Tobago \t\t North America\n",
      "152 Estonia \t\t Europe\n",
      "153 East Timor \t\t Asia\n",
      "154 Mauritius \t\t Africa\n",
      "155 Eswatini \t\t Africa\n",
      "156 Djibouti \t\t Africa\n",
      "157 Cyprus \t\t Asia\n",
      "158 Fiji \t\t Oceania\n",
      "159 Comoros \t\t Africa\n",
      "160 Bhutan \t\t Asia\n",
      "161 Guyana \t\t South America\n",
      "162 Solomon Islands \t\t Oceania\n",
      "– Macau(China) \t\t Asia\n",
      "163 Luxembourg \t\t Europe\n",
      "164 Montenegro \t\t Europe\n",
      "– Western Sahara \t\t Africa\n",
      "165 Suriname \t\t South America\n",
      "166 Cape Verde \t\t Africa\n",
      "167 Malta \t\t Europe\n",
      "168 Belize \t\t North America\n",
      "169 Brunei \t\t Asia\n",
      "170 Bahamas \t\t North America\n",
      "171 Maldives \t\t Asia\n",
      "– Northern Cyprus \t\t Asia\n",
      "172 Iceland \t\t Europe\n",
      "– Transnistria \t\t Europe\n",
      "173 Vanuatu \t\t Oceania\n",
      "174 Barbados \t\t North America\n",
      "– French Polynesia(France) \t\t Oceania\n",
      "– New Caledonia(France) \t\t Oceania\n",
      "– Abkhazia \t\t Asia\n",
      "175 São Tomé and Príncipe \t\t Africa\n",
      "176 Samoa \t\t Oceania\n",
      "177 Saint Lucia \t\t North America\n",
      "– Guam(United States) \t\t Oceania\n",
      "– Curaçao(Netherlands) \t\t North America\n",
      "– Artsakh \t\t Asia\n",
      "178 Kiribati \t\t Oceania\n",
      "179 Grenada \t\t North America\n",
      "– Aruba(Netherlands) \t\t North America\n",
      "180 Saint Vincent and the Grenadines \t\t North America\n",
      "– Jersey(British Crown Dependency) \t\t Europe\n",
      "181 Micronesia \t\t Oceania\n",
      "182 Tonga \t\t Oceania\n",
      "183 Antigua and Barbuda \t\t North America\n",
      "184 Seychelles \t\t Africa\n",
      "– U.S. Virgin Islands(United States) \t\t North America\n",
      "– Isle of Man(British Crown Dependency) \t\t Europe\n",
      "185 Andorra \t\t Europe\n",
      "186 Dominica \t\t North America\n",
      "– Cayman Islands(United Kingdom) \t\t North America\n",
      "– Bermuda(United Kingdom) \t\t North America\n",
      "– Guernsey(British Crown Dependency) \t\t Europe\n",
      "– Greenland(Denmark) \t\t North America\n",
      "187 Marshall Islands \t\t Oceania\n",
      "188 Saint Kitts and Nevis \t\t North America\n",
      "– Faroe Islands(Denmark) \t\t Europe\n",
      "– South Ossetia \t\t Asia\n",
      "– American Samoa(United States) \t\t Oceania\n",
      "– Northern Mariana Islands(United States) \t\t Oceania\n",
      "– Turks and Caicos Islands(United Kingdom) \t\t North America\n",
      "– Sint Maarten(Netherlands) \t\t North America\n",
      "189 Liechtenstein \t\t Europe\n",
      "190 Monaco \t\t Europe\n",
      "– Gibraltar(United Kingdom) \t\t Europe\n",
      "191 San Marino \t\t Europe\n",
      "– Saint Martin(France) \t\t North America\n",
      "– Åland(Finland) \t\t Europe\n",
      "– British Virgin Islands(United Kingdom) \t\t North America\n",
      "192 Palau \t\t Oceania\n",
      "– Cook Islands(New Zealand) \t\t Oceania\n",
      "– Anguilla(United Kingdom) \t\t North America\n",
      "193 Nauru \t\t Oceania\n",
      "– Wallis and Futuna(France) \t\t Oceania\n",
      "194 Tuvalu \t\t Oceania\n",
      "– Saint Barthélemy(France) \t\t North America\n",
      "– Saint Helena, Ascension and Tristan da Cunha(United Kingdom) \t\t Africa\n",
      "– Saint Pierre and Miquelon(France) \t\t North America\n",
      "– Montserrat(United Kingdom) \t\t North America\n",
      "– Falkland Islands(United Kingdom) \t\t South America\n",
      "– Christmas Island(Australia) \t\t Oceania\n",
      "– Norfolk Island(Australia) \t\t Oceania\n",
      "– Niue(New Zealand) \t\t Oceania\n",
      "– Tokelau(New Zealand) \t\t Oceania\n",
      "195 Vatican City \t\t Europe\n",
      "– Cocos (Keeling) Islands(Australia) \t\t Oceania\n"
     ]
    }
   ],
   "source": [
    "for row in rows[2:-1]:\n",
    "    cells = row.find_all([\"td\", \"th\"])\n",
    "    \n",
    "    cells_text = [cell.get_text(strip=True) for cell in cells]\n",
    "    (rank, country, region, population, percentage, updated_at, source, comment) = cells_text  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Attention**: Beautiful Soup does not execute Javascript. This means that you the code in the Google Chrome inspector might look different to the original source code. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Another example of downloading a Wikipedia table \n",
    "\n",
    "Let's consider another table in a Wikipedia page. This page has a lot more tables, so one challenge will be to pick the right table\n",
    "\n",
    "https://en.wikipedia.org/wiki/Tiger_Woods\n",
    "\n",
    "\n",
    "We are interested in extracting these two tables:\n",
    "\n",
    "![Target Wikipedia tables](pictures/wiki_tables.png)\n",
    "\n",
    "**Exercise**: \n",
    "\n",
    "1) Identify the id=\"The_Players_Championship\", by using title = document.find(id=\"The_Players_Championship\")\n",
    "\n",
    "2) First find all tables below the id in 1) by title.find_all_next('table').\n",
    "\n",
    "3) Search for headers (th) by table.find('th') for table in tables to identify the \"Tournament\" header. Remember to use get_text(strip=True)\n",
    "\n",
    "4) Save all tables with the header \"Tournament\" into a list tournament_tables. Check the length of the table and reduce it if it is needed.\n",
    "\n",
    "5) Bonus: Print out the information in the two tables of interest in the terminal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by downloading the webpage and instatiating the BeautifulSoup object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = requests.get(\"https://en.wikipedia.org/wiki/Tiger_Woods\")\n",
    "src = result.content\n",
    "document = BeautifulSoup(src, 'lxml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This page contains a lot of tables without specific attributes that would make it easy to find our table of interest. Further, the same headings of the tables are used for multiple tables, making it difficult to find a table just by its headings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(document.find_all(\"table\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, we choose another strategy. First, we extract the tag that defines the header just before our tables of interest. That header tag has a unique identifier attribute `id=\"The_Players_Championship\"`. Then we use the `find_all_next` function in BeautifulSoup to extract all following table tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = document.find(id=\"The_Players_Championship\")\n",
    "tables = title.find_all_next(\"table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, our tables of interest are the first two tables with the \"Tournament\" heading. We write a small helper function (a generator https://wiki.python.org/moin/Generators) that returns a table with a given heading:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_table_with_heading(tables, heading):\n",
    "    for table in tables:\n",
    "        if table.find(\"th\").get_text(strip=True) == heading:\n",
    "            yield table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can extract the table rows and columns as usual. We only extract the first two tables, as these are the only ones we were interested in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tournament', '1997', '1998', '1999', '2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009']\n",
      "['The Players Championship', 'T31', 'T35', 'T10', '2', '1', 'T14', 'T11', 'T16', 'T53', 'T22', 'T37', '', '8']\n",
      "['Tournament', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018', '2019']\n",
      "['The Players Championship', 'WD', 'WD', 'T40', '1', '', 'T69', '', '', 'T11', 'T30']\n"
     ]
    }
   ],
   "source": [
    "tournament_tables = list(find_table_with_heading(tables, \"Tournament\"))\n",
    "\n",
    "for table in tournament_tables[:2]:\n",
    "    for row in table.find_all(\"tr\"):\n",
    "        cells= row.find_all([\"th\", \"td\"])\n",
    "        print([cell.get_text(strip=\"True\") for cell in cells])\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
