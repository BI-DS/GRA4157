{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Machine learning\n",
    "\n",
    "<img src=\"http://imgs.xkcd.com/comics/tasks.png\">\n",
    "<center>Image credit: xkcd</center>\n",
    "\n",
    "\n",
    "<small><i>This notebook is inspired by and based on the excellent notebooks by [Jake Vanderplas](http://www.vanderplas.com) for PyCon 2015 (source and license info is on [GitHub](https://github.com/jakevdp/sklearn_pycon2015/)) and [Just Markham scikit video lecture](https://github.com/justmarkham/scikit-learn-videos).</i></small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What you will learn today\n",
    "- **Basics in Machine Learning**, and some skills useful in practice.\n",
    "- **Scikit-learn** a Python library for machine learning\n",
    "\n",
    "\n",
    "# What is machine learning?\n",
    "\n",
    "\n",
    "A high level definition is:\n",
    "\n",
    "*\"Machine learning is the semi-automated extraction of knowledge from data\"*  (Just Markham)\n",
    "\n",
    "* Knowledge from data: Starts with a question that might be answerable using data\n",
    "* Automated extraction: A computer provides the insight\n",
    "* Semi-automated: Requires many smart decisions by a human"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Two categories of learning\n",
    "\n",
    "### Supervised learning: Making predictions using data\n",
    "\n",
    "In supervised learning, the machine learns from data with known output (**labels**, **targets**).\n",
    "\n",
    "Examples:\n",
    "<div style=\"display:flex;width:100%\">\n",
    "    <div style=\"width:50%\">\n",
    "Classification:\n",
    "<img src=\"images/classification.png\" width=200px>\n",
    "Example: email spam recognition\n",
    "    </div>\n",
    "        <div>\n",
    "Regression: \n",
    "\n",
    "<img src=\"images/linear_regression.png\" width=200px>\n",
    "Example: Life expectancy as a function of weight and height.\n",
    "        </div>\n",
    "<div />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Unsupervised learning: Extracting structure from data\n",
    "In unsupervised learning, the machine learns from data without data labels.\n",
    "\n",
    "* **Clustering of data**\n",
    "\n",
    "<img src=\"images/clustering.png\" width=300px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Is it supervised or unsupervised learning?\n",
    "\n",
    "Which of these tasks are supervised and which are unsupervised?\n",
    "<div class=\"boxed\" style=\"border: 1px solid green ;\">\n",
    "Given a photograph of a person, identify the person in the photo.\n",
    "</div>\n",
    "<br/>\n",
    "<br/>\n",
    "<!--Supervised (classification)-->\n",
    "<div class=\"boxed\" style=\"border: 1px solid green ;\">\n",
    "Given a list of movies a person has watched and their personal rating of the movie, recommend a list of movies they would like.\n",
    "</div>\n",
    "<br/>\n",
    "Answering this question was worth US$1,000,000 in 2009: [Netflix Prize](http://en.wikipedia.org/wiki/Netflix_prize)\n",
    "<br/>\n",
    "<!--Supervised (regression)-->\n",
    "<div class=\"boxed\" style=\"border: 1px solid green ;\">\n",
    "Given a mixture of two sound sources (for example, a person talking over some music), separate the two (blind source separation problem).\n",
    "</div>\n",
    "<br/>\n",
    "<br/>\n",
    "<!--Unsupervised-->\n",
    "<div class=\"boxed\" style=\"border: 1px solid green ;\">\n",
    "Given a video, isolate a moving object and categorize in relation to other moving objects which have been seen.\n",
    "</div>\n",
    "<br/>\n",
    "<br/>\n",
    "<!--Unsupervised-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How does machine learning work?\n",
    "\n",
    "\n",
    "#### Supervised learning\n",
    "\n",
    "Supervised learning consists of two steps:\n",
    "\n",
    "1. **Model training**: Model is learning the relationship between data and outcome from existing **labeled data** (data with known outcome).\n",
    "2. **Prediction of new data**: The model can predict the outcome for new data.\n",
    "\n",
    "<img src=\"images/plot_ML_flow_chart_1.png\" width=400px alt=\"source: wikipedia\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Important choices when designing :\n",
    "    \n",
    "* Which **features** should be used to train my model?\n",
    "* Which **machine learning algorithm** to use?\n",
    "* **Evaluate the quality** of the prediction model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Scikit-Learn: Machine Learning with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## About Scikit-Learn\n",
    "\n",
    "[Scikit-Learn](http://github.com/scikit-learn/scikit-learn) is a Python module which implements many well-known machine learning algorithms.\n",
    "\n",
    "**Advantages:**\n",
    "* all algorithms accessible from a consistent interface\n",
    "* open-source, large community\n",
    "* sensible defaults settings\n",
    "* good documention\n",
    "* built on numpy and scipy\n",
    "\n",
    "**Disadvantages:**\n",
    "* not optimized for very large datasets\n",
    "* specialised algorithms might not be implemented (yet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Installing scikit-learn\n",
    "\n",
    "If you use anaconda or [miniconda](http://conda.pydata.org/miniconda.html) install with:\n",
    "```\n",
    "$ conda install scikit-learn\n",
    "```\n",
    "\n",
    "If you have do not use anaconda, use instead `pip`:\n",
    "```\n",
    "$ pip install scikit-learn\n",
    "```\n",
    "\n",
    "Check versions of the packages with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also use matplotlib:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Useful Resources\n",
    "\n",
    "- **scikit-learn:** http://scikit-learn.org (see especially the narrative documentation)\n",
    "- **matplotlib:** http://matplotlib.org (see especially the gallery section)\n",
    "- **pandas:** http://pandas.pydata.org (high-performance, easy-to-use data structures and data analysis tools for Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Representation of Data in scikit-learn\n",
    "\n",
    "Machine learning is about creating models from data. Therefore we need to know how to represented the data in order to be understood by the computer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `scikit-learn` expect data to be stored in a **two-dimensional array or matrix**.  \n",
    "1. The array should be a ``numpy`` array.\n",
    "1. The size must be `[n_samples, n_features]`. \n",
    "    - **n_samples:**   The number of samples: each sample is an item to process (e.g. classify).\n",
    "      A sample can be a document, a picture, a sound, a video, an astronomical object,\n",
    "      a row in database or CSV file,\n",
    "      or whatever you can describe with a fixed set of quantitative traits.\n",
    "    - **n_features:**  The number of features or distinct traits that can be used to describe each\n",
    "      item in a quantitative manner.  Features are generally real-valued, but may be boolean or\n",
    "      discrete-valued in some cases.\n",
    "\n",
    "The number of features must be fixed in advance. However it can be very high dimensional\n",
    "(e.g. millions of features) with most of them being zeros for a given sample. For high dimensional, sparse data, use ``scipy.sparse`` matrices instead of `numpy.array`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A first example: Predicting the Iris flower\n",
    "\n",
    "As an example of a simple dataset, we're going to take a look at the\n",
    "iris data stored by scikit-learn.\n",
    "The data consists of measurements of three different species of irises.\n",
    "There are three species of iris in the dataset, which we can picture here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"images/iris_setosa.jpg\" style=\"display: inline-block; height:200px\">\n",
    "<img src=\"images/iris_versicolor.jpg\" style=\"display: inline-block;height:200px\">\n",
    "<img src=\"images/iris_virginica.jpg\" style=\"display: inline-block;height:200px\">\n",
    "</div>\n",
    "Iris Setosa (left), Iris Versicolor (middle), Iris Virginica (right)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal: Predict the species of an Iris using the measurements\n",
    "\n",
    "This is a **supervised machine learning** task. The iris dataset is famous for machine learning because prediction is relatively easy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Question:\n",
    "\n",
    "**If we want to design an algorithm to recognize iris species, what might the data be?**\n",
    "\n",
    "Remember: we need a 2D array of size `[n_samples x n_features]`.\n",
    "\n",
    "- What would the `n_samples` refer to?\n",
    "\n",
    "- What might the `n_features` refer to?\n",
    "\n",
    "Remember that there must be a **fixed** number of (numeric) features for each sample, and feature must be a similar kind of quantity for each sample.\n",
    "\n",
    "(One) solution: \n",
    "\n",
    "\n",
    "<img src=\"https://github.com/justmarkham/scikit-learn-videos/raw/84f03ae1d048482471f2a9ca85b0c649730cc269/images/03_iris.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Loading the Iris Data with Scikit-Learn\n",
    "\n",
    "``scikit-learn`` includes a copy of the iris data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " The data consist of\n",
    "the following:\n",
    "- Features in the Iris dataset:\n",
    " 1. sepal (hanging leaf) length in cm\n",
    " 2. sepal (hanging leaf) width in cm\n",
    " 3. petal (standing leaf) length in cm\n",
    " 4. petal (standing leaf) width in cm\n",
    "\n",
    "- Target labels to predict:\n",
    "  1. Iris Setosa\n",
    "  2. Iris Versicolour\n",
    "  3. Iris Virginica\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the dataset in detail. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "`iris['data']` contains the sample data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris['data']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's see how much data we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris['data'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row represents one flower sample. The meaning of each column is listed in `iris['feature_names']`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris['feature_names']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "`iris['targets']` contains the the target id for each sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris[\"target\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`iris['target_names']` contains the descriptive name for each labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris[\"target_names\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Summary**\n",
    "\n",
    "For training the machine we will use the following data:\n",
    "\n",
    "* `iris[data]`:  150 samples with 4 features each\n",
    "* `iris[target]`:150 labels with 3 label types (*targets*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data is four dimensional, but we can visualize two of the dimensions\n",
    "at a time using a scatter-plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# the two dimensions to use in the plot. Here petal width (x) vs sepal width (y)\n",
    "x_index = 3\n",
    "y_index = 1\n",
    "\n",
    "\n",
    "plt.scatter(iris.data[:, x_index], iris.data[:, y_index],\n",
    "            c=iris.target, cmap=plt.cm.get_cmap('RdYlBu', 3))\n",
    "\n",
    "# this formatter will label the colorbar with the correct target names\n",
    "formatter = plt.FuncFormatter(lambda i, *args: iris.target_names[int(i)])\n",
    "plt.colorbar(ticks=[0, 1, 2], format=formatter)\n",
    "\n",
    "plt.clim(-0.5, 2.5)\n",
    "plt.xlabel(iris.feature_names[x_index])\n",
    "plt.ylabel(iris.feature_names[y_index]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training the model\n",
    "\n",
    "The setup:\n",
    "* 150 observations\n",
    "* 4 features (sepal length, sepal width, petal length, petal width)\n",
    "* Label is the iris species\n",
    "* Classification problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## K-nearest neighbors (KNN) classification \n",
    "\n",
    "**KNN algorithm**\n",
    "\n",
    "*Input*: New iris features. \n",
    "\n",
    "*Out*: Iris target\n",
    "\n",
    "* Pick a value for K.\n",
    "* Search for the K neighbours in the sample data that are nearest to the new iris.\n",
    "* Return the most popular target value from the K nearest neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Dataset**\n",
    "<br>\n",
    "<img src=\"images/knn_1.png\" alt=\"source:wikipedia\" width=\"400px\"> <br>\n",
    "**1-Nearest neighbours classification** <br>\n",
    "<img src=\"images/knn_2.png\" alt=\"source:wikipedia\" width=\"400px\"> <br>\n",
    "**5-Nearest neighbours classification** <br>\n",
    "<img src=\"images/knn_3.png\" alt=\"source:wikipedia\" width=\"400px\"> <br>\n",
    "White areas consists of the points where the vote is tight (no most popular target value) <br>\n",
    "<img src=\"images/overfitting_2.png\" width=\"900px\"> <br>\n",
    "Example of underfitting, appropriate fitting and overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## scikit-learn in 4 steps\n",
    "\n",
    "**Step 1**: Import the class you plan to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2**: Instantiate an `Estimator`, scikit-learn's term for model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could tune the model by changing the default parameters.\n",
    "All parameters not specified are set to their defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Step 3**: Model training, i.e. fit the model with data. Model is learning the relationship between iris.data and iris.target. This step occurs in-place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.fit(iris.data, iris.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4**: Predict the response for a new observation. \n",
    "New observations are called \"out-of-sample\" data.\n",
    "Uses the information it learned during the model training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What kind of iris has 3cm x 5cm sepal and 4cm x 2cm petal?\n",
    "# call the \"predict\" method:\n",
    "\n",
    "knn.predict([[3, 5, 4, 2],])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It returns a NumPy array. We can map this to the label name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris.target_names[_])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "Can predict for multiple observations at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_new = [[3, 5, 4, 2], [5, 4, 3, 2]]\n",
    "knn.predict(iris_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also do probabilistic predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.predict_proba([[3, 5, 4, 2],])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or create a classification map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from fig_code import plot_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "iris = load_iris()\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(iris.data, iris.target)\n",
    "pred = knn.predict([[3, 5, 4, 2],])\n",
    "print(pred)\n",
    "\n",
    "plot_iris(knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Exercise\n",
    "\n",
    "1) Run through the 4 steps of KNeighbors with the iris data\n",
    "\n",
    "2) Vary the size of n_neighbors and inspect the results. \n",
    "\n",
    "3) Create your own data and target (e.g. data = [1, 1.1, 0.9, 2, 1.9, 2.1], target = [0,0,0,1,1,1]) and run predictions for a few numbers. For data with only 1 feature you might want to pass data.reshape(-1, 1) into the knn.fit -step.\n",
    "\n",
    "4) What would you expect from knn.predict([[1.5],])? Try to think of it before testing the code. Also try to call knn.predict_proba on the same data point. \n",
    "\n",
    "5) Try with a different classifier (for instance SVC from sklearn.svm). You can find many more classifier under \"classifier comparison\" on scikit-learn.org (just google scikit learn classifier comparison). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Changing the value of K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We crate a new classifier, fit the data and perform the prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the model (using the value K=5)\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# fit the model with data\n",
    "knn.fit(iris.data, iris.target)\n",
    "\n",
    "# predict the response for new observations\n",
    "knn.predict([[3, 5, 4, 2], [5, 4, 3, 2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plot_iris(knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Using a different Classification algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the class\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# instantiate the model (using the default parameters)\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# fit the model with data\n",
    "logreg.fit(iris.data, iris.target)\n",
    "\n",
    "# predict the response for new observations\n",
    "logreg.predict(iris_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_iris(logreg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Flow Chart: How to Choose your Estimator\n",
    "\n",
    "This is a flow chart created by scikit-learn super-contributor [Andreas Mueller](https://github.com/amueller) which gives a nice summary of which algorithms to choose in various situations. Keep it around as a handy reference!\n",
    "\n",
    "<img src=\"http://scikit-learn.org/dev/_static/ml_map.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Exercise\n",
    "In this exercise, you will hard-code the KNearest Neighbor algorithm. \n",
    "\n",
    "Consider the data points <br>\n",
    "`data = array([[1,1], [1,0], [0.4,0.8], [2,5], [2,4], [2.2, 4.6]])` with targets: <br>\n",
    "`targets = array([0,0,0,1,1,1]`\n",
    "\n",
    "1) To display our data, let the target correspond to the colors ['blue', 'red'] and plot each data-point in the xy-plane with the correct color\n",
    "\n",
    "2) Make a function `distance(p0, P1)` that returns the (Eucledian) distance between a point p0 (the point to predict) to each point in the array of points P1 (the data). Calling e.g. `distance(array([1,1]),array([[1,2],[2,2]])` should return [1, sqrt(2)] \n",
    "\n",
    "Now set `p_pred = array([1.75, 2])` as a point we would like to predict the color of\n",
    "\n",
    "3) Make a sorted list of the distance between the data-points and the point to predict (p_pred). Now you should find out what colors the closest points are. Set n as an adjustable parameter and find the color of the n closest points. (hint: numpy argsort could be useful)\n",
    "\n",
    "4) Let the n closest points give their vote on what the predicted target (and thus color) should be (bincount, and argmax could be useful)\n",
    "\n",
    "5) Compare the result from your code with  \n",
    "\n",
    "```from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=n)\n",
    "knn.fit(data, targets)\n",
    "knn.predict([p_pred,])\n",
    "```\n",
    "\n",
    "And play around with the value of n. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Step 1: Plot data points\n",
    "data = np.array([[1, 1], [1, 0], [0.4, 0.8], [2, 5], [2, 4], [2.2, 4.6]])\n",
    "targets = np.array([0, 0, 0, 1, 1, 1])\n",
    "colors = ['blue' if t == 0 else 'red' for t in targets]\n",
    "\n",
    "plt.scatter(data[:, 0], data[:, 1], c=colors)\n",
    "plt.title(\"Data Points\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"https://www.gstatic.com/education/formulas2/553212783/en/euclidean_distance.svg\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Create distance function\n",
    "def distance(p0, P1):\n",
    "    return np.sqrt(np.sum((P1 - p0)**2, axis=1))\n",
    "\n",
    "distance(np.array([1, 1]), np.array([[1,2],[2,2]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Find closest points\n",
    "p_pred = np.array([1.75, 2])\n",
    "n = 3  # adjustable parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([[1, 1], [1, 0], [0.4, 0.8], [2, 5], [2, 4], [2.2, 4.6]])\n",
    "targets = np.array([0, 0, 0, 1, 1, 1])\n",
    "colors = ['blue' if t == 0 else 'red' for t in targets]\n",
    "\n",
    "plt.scatter(data[:, 0], data[:, 1], c=colors)\n",
    "plt.scatter(p_pred[0], p_pred[0], c=\"green\")\n",
    "plt.title(\"Data Points\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = distance(p_pred, data)\n",
    "closest_n_indices = []\n",
    "for i in range(n):\n",
    "    min_index = np.argmin(distances)\n",
    "    closest_n_indices.append(min_index)\n",
    "    distances[min_index] = float('inf')  # set to inf to avoid picking it again\n",
    "\n",
    "closest_n_colors = [targets[i] for i in closest_n_indices]\n",
    "\n",
    "closest_n_colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Voting\n",
    "votes = {0: 0, 1: 0}\n",
    "for color in closest_n_colors:\n",
    "    votes[color] += 1\n",
    "\n",
    "predicted_target = max(votes, key=votes.get)\n",
    "\n",
    "# Step 5: Compare with sklearn\n",
    "knn = KNeighborsClassifier(n_neighbors=n)\n",
    "knn.fit(data, targets)\n",
    "sklearn_prediction = knn.predict([p_pred,])\n",
    "\n",
    "print(f\"Manual KNN Prediction: {predicted_target}\")\n",
    "print(f\"Sklearn KNN Prediction: {sklearn_prediction[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Evaluating the performance of a prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating the performane of a prediction is crucial in order to:\n",
    "* choose which model to use for my supervised learning task;\n",
    "* cchoose the best tuning parameters for that model;\n",
    "* estimate the likely performance of my model on out-of-sample data;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## **Training accuracy**: Train and test on the entire dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following approach is used to compute **training accucarcy**:\n",
    "     \n",
    "1. Train the model on the entire dataset.\n",
    "1. Test the model on the same dataset, and evaluate how well we did by comparing the predicted response values with the true response values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(max_iter=200)\n",
    "\n",
    "# fit the model with data\n",
    "logreg.fit(iris.data, iris.target)\n",
    "pred = logreg.predict(iris.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scikit provides the function `metrics.accuracy_score` to compute the **training accuracy**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "metrics.accuracy_score(iris.target, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It tells us that we predicted 97% of the predicted samples with the correct label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test some more:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(iris.data, iris.target)\n",
    "pred_target = knn.predict(iris.data)\n",
    "metrics.accuracy_score(iris.target, pred_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(iris.data, iris.target)\n",
    "pred_target = knn.predict(iris.data)\n",
    "metrics.accuracy_score(iris.target, pred_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Evaluation based on cross validation\n",
    "\n",
    "Idea:\n",
    "1. Split the sample dataset into two pieces: a training set and a testing set.\n",
    "1. Train the model on the training set.\n",
    "1. Test the model on the testing set, and evaluate the accuracy of the prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Step 1: Split data set into training and testing sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`scikit` learn provides the function `train_test_split` to split up the saple test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Randomly split iris.data and iris.target into training and testing sets\n",
    "data_train, data_test, target_train, target_test = train_test_split(iris.data, iris.target, test_size=0.4, random_state=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's check what we got:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Step 2: Fit model using training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(data_train, target_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Step 3: Predict target values for testing values, evaluate performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we predict the target values for the test samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.predict(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we evaluate the performance of the prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.accuracy_score(target_test, _)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### We can now test the performance of the prediction for different classification algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_range = list(range(1, 26))\n",
    "scores = []\n",
    "for k in k_range:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(data_train, target_train)\n",
    "    target_pred = knn.predict(data_test)\n",
    "    scores.append(metrics.accuracy_score(target_test, target_pred))\n",
    "\n",
    "# plot the relationship between K and testing accuracy\n",
    "plt.plot(k_range, scores)\n",
    "plt.xlabel('Value of K for KNN')\n",
    "plt.ylabel('Testing Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Unsupervised Learning: Dimensionality Reduction and Clustering\n",
    "\n",
    "**Unsupervised Learning** addresses a different sort of problem. Here the data has no labels,\n",
    "and we are interested in finding similarities between the objects in question. In a sense,\n",
    "you can think of unsupervised learning as a means of discovering labels from the data itself.\n",
    "Unsupervised learning comprises tasks such as *dimensionality reduction*, *clustering*, and\n",
    "*density estimation*. For example, in the iris data discussed above, we can used unsupervised\n",
    "methods to determine combinations of the measurements which best display the structure of the\n",
    "data. As we'll see below, such a projection of the data can be used to visualize the\n",
    "four-dimensional dataset in two dimensions. Some more involved unsupervised learning problems are:\n",
    "\n",
    "- given detailed observations of distant galaxies, determine which features or combinations of\n",
    "  features best summarize the information.\n",
    "- given a mixture of two sound sources (for example, a person talking over some music),\n",
    "  separate the two (this is called the [blind source separation](http://en.wikipedia.org/wiki/Blind_signal_separation) problem).\n",
    "- given a video, isolate a moving object and categorize in relation to other moving objects which have been seen.\n",
    "\n",
    "Sometimes the two may even be combined: e.g. Unsupervised learning can be used to find useful\n",
    "features in heterogeneous data, and then these features can be used within a supervised\n",
    "framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Dimensionality Reduction: PCA\n",
    "\n",
    "Principle Component Analysis (PCA) is a dimension reduction technique that can find the combinations of variables that explain the most variance.\n",
    "\n",
    "Consider the iris dataset. It cannot be visualized in a single 2D plot, as it has 4 features. We are going to extract 2 combinations of sepal and petal dimensions to visualize it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "\n",
    "X, y = iris.data, iris.target\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X)\n",
    "X_reduced = pca.transform(X)\n",
    "print(\"Reduced dataset shape:\", X_reduced.shape)\n",
    "\n",
    "import pylab as pl\n",
    "pl.scatter(X_reduced[:, 0], X_reduced[:, 1], c=y,\n",
    "           cmap='RdYlBu')\n",
    "\n",
    "print(\"Meaning of the 2 components:\")\n",
    "for component in pca.components_:\n",
    "    print(\" + \".join(\"%.3f x %s\" % (value, name)\n",
    "                     for value, name in zip(component,\n",
    "                                            iris.feature_names)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Clustering: K-means\n",
    "\n",
    "Clustering groups together observations that are homogeneous with respect to a given criterion, finding ''clusters'' in the data.\n",
    "\n",
    "Note that these clusters will uncover relevant hidden structure of the data only if the criterion used highlights it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "k_means = KMeans(n_clusters=3, random_state=0) # Fixing the RNG in kmeans\n",
    "k_means.fit(X)\n",
    "y_pred = k_means.predict(X)\n",
    "\n",
    "pl.scatter(X_reduced[:, 0], X_reduced[:, 1], c=y_pred,\n",
    "           cmap='RdYlBu');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Quick Application: Optical Character Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate the above principles on a more interesting problem, let's consider OCR (Optical Character Recognition) – that is, recognizing hand-written digits.\n",
    "In the wild, this problem involves both locating and identifying characters in an image. Here we'll take a shortcut and use scikit-learn's set of pre-formatted digits, which is built-in to the library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Loading and visualizing the digits data\n",
    "\n",
    "We'll use scikit-learn's data access interface and take a look at this data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "digits = datasets.load_digits()\n",
    "digits.images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's plot a few of these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(10, 10, figsize=(8, 8))\n",
    "fig.subplots_adjust(hspace=0.1, wspace=0.1)\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(digits.images[i], cmap='binary')\n",
    "    ax.text(0.05, 0.05, str(digits.target[i]),\n",
    "            transform=ax.transAxes, color='green')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here the data is simply each pixel value within an 8x8 grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The images themselves\n",
    "print(digits.images.shape)\n",
    "print(digits.images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data for use in our algorithms\n",
    "print(digits.data.shape)\n",
    "print(digits.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The target label\n",
    "print(digits.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our data have 1797 samples in 64 dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Unsupervised Learning: Dimensionality Reduction\n",
    "\n",
    "We'd like to visualize our points within the 64-dimensional parameter space, but it's difficult to plot points in 64 dimensions!\n",
    "Instead we'll reduce the dimensions to 2, using an unsupervised method.\n",
    "Here, we'll make use of a manifold learning algorithm called *Isomap*, and transform the data to two dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import Isomap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "iso = Isomap(n_components=2, n_neighbors=10)\n",
    "data_projected = iso.fit_transform(digits.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_projected.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data_projected[:, 0], data_projected[:, 1], c=digits.target,\n",
    "            edgecolor='none', alpha=0.5, cmap=plt.cm.get_cmap('nipy_spectral', 10));\n",
    "plt.colorbar(label='digit label', ticks=range(10))\n",
    "plt.clim(-0.5, 9.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see here that the digits are fairly well-separated in the parameter space; this tells us that a supervised classification algorithm should perform fairly well. Let's give it a try."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Classification on Digits\n",
    "\n",
    "Let's try a classification task on the digits. The first thing we'll want to do is split the digits into a training and testing sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(digits.data, digits.target,\n",
    "                                                random_state=2)\n",
    "print(Xtrain.shape, Xtest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use a simple logistic regression which (despite its confusing name) is a classification algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(max_iter=200)\n",
    "clf.fit(Xtrain, ytrain)\n",
    "ypred = clf.predict(Xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can check our classification accuracy by comparing the true values of the test set to the predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(ytest, ypred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This single number doesn't tell us **where** we've gone wrong: one nice way to do this is to use the *confusion matrix*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(ytest, ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.log(confusion_matrix(ytest, ypred)),\n",
    "           cmap='Blues', interpolation='nearest')\n",
    "plt.grid(False)\n",
    "plt.ylabel('true')\n",
    "plt.xlabel('predicted');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We might also take a look at some of the outputs along with their predicted labels. We'll make the bad labels red:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(10, 10, figsize=(8, 8))\n",
    "fig.subplots_adjust(hspace=0.1, wspace=0.1)\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(Xtest[i].reshape(8, 8), cmap='binary')\n",
    "    ax.text(0.05, 0.05, str(ypred[i]),\n",
    "            transform=ax.transAxes,\n",
    "            color='green' if (ytest[i] == ypred[i]) else 'red')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The interesting thing is that even with this simple logistic regression algorithm, many of the mislabeled cases are ones that we ourselves might get wrong!\n",
    "\n",
    "There are many ways to improve this classifier, but we're out of time here. To go further, we could use a more sophisticated model, use cross validation, or apply other techniques."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
